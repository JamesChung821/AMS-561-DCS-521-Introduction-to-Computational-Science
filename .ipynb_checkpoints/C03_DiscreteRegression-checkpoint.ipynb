{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete Regression\n",
    "\n",
    "Here we consider *discrete* dependent variables: binary outcomes, variables taking positive integer values. For details and examples read [here](https://www.kellogg.northwestern.edu/faculty/dranove/htm/dranove/coursepages/Mgmt%20469/discrete-lhs.pdf).\n",
    "\n",
    "Linear regression requires that the dependent(response) variable is a normal-distributed *continuous* variable and thus cannot be used directly for discrete dependent variables.\n",
    "\n",
    "However, it is possible to map a linear predictor to an interval that can be considered as a *probability* of discrete outcome.  \n",
    "\n",
    "In a bit more detail:\n",
    "* We want to estimate the probability ($p$) of observing $y$ given the value of the independent variable $x$.  \n",
    "* Simply fitting a linear model will in general fail since probabilities must be positive and sum to one.\n",
    "* So we need a non-linear model, but we already know this can be hard.\n",
    "* However, instead of fitting the probability directly, we use a linear model to fit a known non-linear function of the probability.  Once this function is fitted, we can then compute the probability.\n",
    "\n",
    "The `statsmodels` library has several classes for handling discrete regression:\n",
    "- `Logit` : binary choice logit model (we'll use this below)\n",
    "- `Probit`: binary choice Probit model uses a cumulative distribution function of the normal distribution (the error function or `erf`) instead of the logistic function, but is in spirit very similar to `Logit`\n",
    "- `MNLogit`: multinomial logistic regression (for more than 2 categories) \n",
    "- `Poisson`: Possion regression (for count data)\n",
    "\n",
    "For more detail, see http://www.statsmodels.org/dev/discretemod.html and https://www.statsmodels.org/stable/index.html \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm \n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.graphics.api as smg\n",
    "import patsy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For binary outcomes, a popular map is the logistic function \n",
    "$$\n",
    "\\log \\frac{p}{1-p}\n",
    "$$\n",
    "where $p$ is the probability of an outcome.  \n",
    "\n",
    "We will fit this function of the probability to a linear model, i.e., \n",
    "$$\n",
    "\\log \\frac{p}{1-p}  = \\beta_0 + \\beta_1 x\n",
    "$$\n",
    "where $\\beta_0$ and $\\beta_1$ are the model parameters, and $x$ is the independent variable(s).\n",
    "\n",
    "*Note* that the right hand side is *linear* in the parameters $\\beta_0$ and $\\beta_1$ so we can use linear regression to fit $\\log \\frac{p}{1-p}$.  Once we have fitted this, we can rearrange to compute the probability with \n",
    "$$\n",
    "     p(x) = \\frac{1}{1+exp(-\\beta_0 - \\beta_1 x)}.\n",
    "$$\n",
    "* This is called the logistic (or sigmoid) function.\n",
    "* It maps $x \\in [-\\infty, \\infty]$ to $ p \\in [0,1] $.\n",
    "* It takes the value $p=0.5$ at $x=-\\frac{\\beta_0}{\\beta_1}$.\n",
    "* The rate of switching (sharpness of classification) is controlled by $\\beta_1$.\n",
    "\n",
    "To make a binary classifier that selects between $y = 0,1$\n",
    "* If $p<0.5$, we predict that $y=0$, and\n",
    "* If $p\\geq 0.5$, we predict that $y=1$.   \n",
    "\n",
    "Play with $\\beta_0$ and $\\beta_1$ below to get some intuition about what is going in.\n",
    "* Smaller beta1 means slower switching and hence less sharp classification\n",
    "* Large beta1 means faster switching and hence sharper classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given beta0 and beta1 plot the logistic function \n",
    "\n",
    "beta1 = 0.5 # Try adjusting the rate of switching from 0.1 to 10\n",
    "beta0 = -5.0*beta1\n",
    "xhalf = -beta0/beta1\n",
    "x = np.linspace(-10,20,300)\n",
    "p = 1.0/(1.0+np.exp(-beta0 - beta1*x))\n",
    "# Plot the logistic function along with lines to mark the transition point\n",
    "plt.plot(x,p,\"b-\",[xhalf,xhalf], [0,1], \"k\",[-10,20],[0.5,0.5],\"k--\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"p(x)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to check version of Statmodels\n",
    "#sm.show_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "[Logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) is an important *machine-learning* technique estimates the parameters for a binary classification model that assigns a probability score to select between two possible outcomes.\n",
    "\n",
    "Briefly review Wikipedia page (above link).\n",
    "\n",
    "Advantages:\n",
    "* doesn’t require high computational power\n",
    "* easily interpretable\n",
    "* widely used\n",
    "* easy to implement\n",
    "* doesn’t require scaling of features\n",
    "* provides a probability score for observations.\n",
    "\n",
    "Disadvantages:\n",
    "* cannot handle a large number of categorical features/variables\n",
    "* vulnerable to overfitting\n",
    "* cannot solve a non-linear problem with the logistic regression model (must first transform them)\n",
    "* will not perform well with independent (X) variables that are not correlated to the target(Y) variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** We will load a dataset which contains sepal and petal lengths and widths for a sample of [Iris flowers](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html) and a classification of the species of the flower.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1vt_4kXCaIPTiTkA7nQzdnXhg792MZHUi\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the \"iris\" dataset \n",
    "df = sm.datasets.get_rdataset(\"iris\").data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the data\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique types of species in the Species column\n",
    "df.Species.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 species. \n",
    "\n",
    "To get a binary variable, we will choose 2 species: 'versicolor', 'virginica'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract subset\n",
    "df_subset = df[(df.Species==\"versicolor\") | (df.Species==\"virginica\")].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at info\n",
    "df_subset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at data\n",
    "df_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are doing binary classification so we must choose one species to be 0 and the other to be 1\n",
    "* Versicolor --- 1\n",
    "* Virginica --- 0\n",
    "\n",
    "Replace the text species label with the integer label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the two species names into binary values 0 and 1\n",
    "# !!! Don't execute this cell twice ... if you do you will see NaN\n",
    "# and must reload the data and reexecute everything down to here\n",
    "\n",
    "df_subset.Species = df_subset.Species.map({\"versicolor\":1, \"virginica\":0})\n",
    "df_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot (\".\") in the column names means we cannot use the column name as an accessor for the data\n",
    "(and hence breaks Patsy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.Sepal.Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we replace the \".\" with an \"_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \" . \" in column names with \" _ \"\n",
    "df_subset.rename(columns={\"Sepal.Length\": \"Sepal_Length\", \"Sepal.Width\": \"Sepal_Width\",\n",
    "                          \"Petal.Length\": \"Petal_Length\", \"Petal.Width\": \"Petal_Width\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrate use of accessor, note the name is case sensitive\n",
    "df_subset.Sepal_Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have a DataFrame object that is ready for use for analysis\n",
    "df_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the data visually trying to see which variables correlate strongly with outcome\n",
    "\n",
    "There are 16 possible combinations of our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Petal_Length\",\"Petal_Width\",\"Sepal_Length\", \"Sepal_Width\"]\n",
    "data0 = [df_subset[df_subset.Species == 0][name].values for name in names]\n",
    "data1 = [df_subset[df_subset.Species == 1][name].values for name in names]\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "count = 1\n",
    "for i in range(len(names)):\n",
    "    for j in range(len(names)):\n",
    "        ax = fig.add_subplot(4,4,count)\n",
    "        ax.plot(data0[i],data0[j],'s', label='virginica')\n",
    "        ax.plot(data1[i],data1[j],'s', label='versicolor')\n",
    "        ax.set_xlabel(names[i])\n",
    "        ax.set_ylabel(names[j])\n",
    "        ax.legend(loc=2)\n",
    "        count += 1\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow --- look at the diagonal plots --- perfect separation seems possible!\n",
    "* But why are there so few virginica points?  E.g., look at the off diagonal plots. \n",
    "* What's going on?\n",
    "\n",
    "Recall the mantras \"guilty until proved innocent\" and \"too good to be true.\"\n",
    "\n",
    "Redo the plot but change the markers for data2 to '.'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,12))\n",
    "count = 1\n",
    "for i in range(len(names)):\n",
    "    for j in range(len(names)):\n",
    "        ax = fig.add_subplot(4,4,count)\n",
    "        ax.plot(data0[i],data0[j],'s', label='virginica')\n",
    "        ax.plot(data1[i],data1[j],'.', label='versicolor')\n",
    "        ax.set_xlabel(names[i])\n",
    "        ax.set_ylabel(names[j])\n",
    "        ax.legend(loc=2)\n",
    "        count += 1\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the moral? \n",
    "* Be very careful with plots --- they can easily mislead\n",
    "* Keep asking questions to support your conclusions\n",
    "* Even \"correct\" programs can produce \"incorrect\" results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do a logistic regression analysis.\n",
    "\n",
    "Documentation is [here](https://www.statsmodels.org/stable/generated/statsmodels.formula.api.logit.html#statsmodels.formula.api.logit)\n",
    "\n",
    "We start off trying to use all four measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the smf.logit class with a Patsy formula \n",
    "model = smf.logit(\"Species ~ Sepal_Length + Sepal_Width + Petal_Length + Petal_Width\"\n",
    "                 , data = df_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the fit method to fit the model\n",
    "result = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a summary of the model\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $z$-score is the regression coefficient divided by the standard coefficient --- it is the number of standard deviations the coefficient is away from 0\n",
    "- If $|z|$ is very large (cut-off value ~ 2), then the corresponding coefficient is significant (not likely to be 0).\n",
    "\n",
    "We can already see that the sepal lengths and widths have less significance.\n",
    "\n",
    "An analysis of the marginal effects sheds more light"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the marginal effects of each dependent variable\n",
    "print(result.get_margeff().summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marginal effects for discrete variables measure how predicted probabilities change as the binary independent variable changes from 0 to 1. \n",
    "\n",
    "For our model, Sepal_Length and Sepal_Width don't seem to contribute much to predictiveness of the model (small z-statistic and large p-value, especially relative to those for the petal length/width).\n",
    "\n",
    "In general, reducing the number of parameters can make a model more interpretable and possibly more predictive.\n",
    "\n",
    "**Exercise: Create and fit a new model using only `Petal_Length` and `Petal_Width`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the smf.logit class with a Patsy formula\n",
    "model = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the fit method to fit the model\n",
    "result  = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a summary of the model\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the marginal effects of each dependent variable\n",
    "print(result.get_margeff().summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this model to make a prediciton on random independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data frame with new values of independent variables\n",
    "df_new = pd.DataFrame({\"Petal_Length\": np.random.randn(20)*0.5 + 5,\n",
    "                       \"Petal_Width\": np.random.randn(20)*0.5 + 1.7})\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the 'predict' method to make a prediction\n",
    "df_new[\"P_Species\"] = result.predict(df_new) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the top 5 rows of df_new\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the probability > 0.5, we assign 1 to the variable\n",
    "df_new[\"Species\"] = (df_new[\"P_Species\"] > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the top 5 rows of df_new again \n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intercept and the slope of the line in the plane spanned by 'Petal_Width' and 'Petal_Length' that defines the boundary of our classification can be computed from the fitted model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = result.params\n",
    "beta0 = -params['Intercept']/params['Petal_Width']\n",
    "beta1 = -params['Petal_Length']/params['Petal_Width']\n",
    "beta0, beta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a scatter plot of the fitted (squares) and the predicted (circles) data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "ax.plot(df_subset[df_subset.Species == 0].Petal_Length.values,\n",
    "        df_subset[df_subset.Species == 0].Petal_Width.values, 's', label='virginica')\n",
    "ax.plot(df_new[df_new.Species == 0].Petal_Length.values,\n",
    "        df_new[df_new.Species == 0].Petal_Width.values,\n",
    "        'o', markersize=10, color=\"steelblue\", label='virginica (pred.)')\n",
    "\n",
    "ax.plot(df_subset[df_subset.Species == 1].Petal_Length.values,\n",
    "        df_subset[df_subset.Species == 1].Petal_Width.values, 's', label='versicolor')\n",
    "ax.plot(df_new[df_new.Species == 1].Petal_Length.values,\n",
    "        df_new[df_new.Species == 1].Petal_Width.values,\n",
    "        'o', markersize=10, color=\"green\", label='versicolor (pred.)')\n",
    "\n",
    "_x = np.array([4.0, 6.1])\n",
    "ax.plot(_x, beta0 + beta1 * _x, 'k')\n",
    "\n",
    "ax.set_xlabel('Petal length')\n",
    "ax.set_ylabel('Petal width')\n",
    "ax.legend(loc=2)\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson Model\n",
    "\n",
    "\n",
    "The [Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution) describes a process where the discrete (0,1,2,...) dependent variable counts the number of occurrences (0, 1, 2, ...) per unit time (or space) for a process that has a low constant probability of occuring.  A constant rate also implies that the model assumes occurrences are independent.\n",
    "\n",
    "The parameter that needs estimating is the rate (number of occurrences per unit time) which is often called $\\lambda$ (lambda).\n",
    "\n",
    "Briefly look at the Wikipedia page (above link).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** Consider 'discoveries' dataset which contains counts of the number of great discoveries between 1860 to 1959:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from the R dataset repository\n",
    "dataset = sm.datasets.get_rdataset(\"discoveries\")\n",
    "dataset.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data.info()\n",
    "dataset.data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame with index \"time\"\n",
    "df = dataset.data.set_index(\"time\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the column name \"value\" to \"discoveries\" for reability\n",
    "df.rename(columns={\"value\": \"discoveries\"}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data --- a bar chart or histogram seems appropriate for this type of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(16, 4))\n",
    "df.plot(kind='bar', ax=ax)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the hypothesis that the number of discoveries is [Poisson distributed](https://en.wikipedia.org/wiki/Poisson_distribution). Let's check this hypothesis.\n",
    "\n",
    "Documentation for the statsmodel Poisson model is [here](https://www.statsmodels.org/stable/generated/statsmodels.discrete.discrete_model.Poisson.html#statsmodels.discrete.discrete_model.Poisson)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the model\n",
    "model = smf.poisson(\"discoveries ~ 1\", data = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit\n",
    "result = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print summary\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or a little more detail\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\lambda$ parameter of the [Poisson](https://en.wikipedia.org/wiki/Poisson_distribution) distribution can be calculated from the model parameter:\n",
    "\n",
    "(note that lambda is a reserved keyword in Python, so cannot use that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmbda = np.exp(result.params)\n",
    "print(lmbda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard error (deviation) enables us to construct an interval for $\\lambda$ in which we have 95% (or pick your own value) confidence that the values should correspond. \n",
    "* 95% corresponds to $\\pm$1.96 standard deviations.  \n",
    "\n",
    "For the intercept the confidence interval is computed by hand as follows (values also provided in the table above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute by hand\n",
    "1.1314-1.96*0.0568 , 1.1314+1.96*0.0568"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it easier, the statsmodel package provides a routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use result.conf_int\n",
    "result.conf_int().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the confidence interval for the intercept into an interval for the predicted value of lambda\n",
    "import math\n",
    "lmbda_lo = math.exp(result.conf_int().values[0,0])\n",
    "lmbda_hi = math.exp(result.conf_int().values[0,1])\n",
    "print(lmbda_lo, lmbda_hi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The counts predicted by this model can now be obtained from the Scipy `stats` library:\n",
    "\n",
    "First construct a Poisson model with the given $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = stats.poisson(lmbda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models for the lower and upper bounds of the confidence interval \n",
    "X_ci_lo = stats.poisson(lmbda_lo)\n",
    "X_ci_hi = stats.poisson(lmbda_hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of $k$ events ocurring per unit time is called the probability mass function (the analog of probability distribution function for continuous distributions).\n",
    "\n",
    "Now let's make a plot showing the probability mass function for\n",
    "* the original data\n",
    "* the fit computed with `lmbda`\n",
    "* the fit computed with `lmbda_lo`\n",
    "* the fit computed with `lmbda_hi`\n",
    "The messy bit is configuring the bars to make all of the data visible at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the histogram of the observed counts with the theoretical probablity mass functions \n",
    "v, k = np.histogram(df.values, bins=12, range=(0, 12), density=True)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "ax.bar(k[:-1], v, color=\"steelblue\",  align='center', label='Dicoveries per year') \n",
    "ax.bar(k-0.125, X_ci_lo.pmf(k), color=\"red\", alpha=0.5, align='center', width=0.25, label='Poisson fit (CI, lower)')\n",
    "ax.bar(k, X.pmf(k), color=\"green\",  align='center', width=0.5, label='Poisson fit')\n",
    "ax.bar(k+0.125, X_ci_hi.pmf(k), color=\"red\",  alpha=0.5, align='center', width=0.25, label='Poisson fit (CI, upper)')\n",
    "\n",
    "ax.legend()\n",
    "#fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the Poisson model seems to predict the overall shape and trends.\n",
    "\n",
    "However, for multiple values of $k$ the observed counts lie outside the 95% confidence intervals --- the theoretical Poisson probabilty mass functions do not agree well in detail. Thus, we should reject the hypothesis that the discoveries are Poisson distributed. A possible reason why this data is not Poisson distributed is that years with a large and small number of discoveries tend to be clustered --- i.e., the events are correlated for some reason (economic/social events, one bright person, chains of ideas, ...). To make this decision (reject the model) more rigorous we could apply a $\\chi^2$-test, but that is beyond the scope of this class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References: \n",
    "- *Numerical Python: A Practical Techniques Approach for Industry*  by Robert Johansson (Chapter 14)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
