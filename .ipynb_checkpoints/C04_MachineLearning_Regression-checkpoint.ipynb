{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "Machine learning is closely related to statistical modeling in the sense that they both use data to predict outcomes of unknown processes.  Statistical modeling focuses on understanding how the data is generated while machine learning focuses on algorithms that can be trained with fewer assumptions on the distribution and statistical properties on the data. We give the models tunable parameters that can be adapted to observed data; the program is \"learning\" from the data.\n",
    "\n",
    "### Categories of Machine Learning\n",
    "1. *Supervised learning*: Data is labeled and the algorithms learn to predict the output(responses) from the input data. \n",
    "    - classification: when the output is a category, such as \"disease\" and \"no disease\"\n",
    "    - regression: when the output is continuous, such as \"length\"\n",
    "2. *Unsupervised Learning*: Data is unlabeled and the algorithms learn to inherent structure from the input data\n",
    "    - clustering: want to find inherent groupings in data, e.g., grouping customers by buying behavior\n",
    "    - dimensionality reduction: search for more succinct representations of data\n",
    "\n",
    "![Supervised_Unsupervised](https://www.mathworks.com/help/stats/machinelearning_supervisedunsupervised.png)\n",
    "(Picture from https://www.mathworks.com/help/stats/machine-learning-in-matlab.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn \n",
    "One of the best known machine learning libraries for Python is [Scikit-Learn](http://scikit-learn.org/stable/). Scikit-Learn has a clean and uniform API as well as very complete online documentation.  Some other (less-popular?) machine learning libraries include [mlpy](http://mlpy.sourceforge.net/) and [PyBrain](http://pybrain.org/). And there are massive projects such as [Keras](https://keras.io/) and [TensorFlow](https://opensource.google/projects/tensorflow) and [PyTorch](https://pytorch.org/) which are recommended for larger projects once you have some understanding especially of neural networks.  Here we will work with Scikit-Learn only.\n",
    "\n",
    "Let's explicitly import modules from the library that we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import model_selection \n",
    "from sklearn import linear_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns #for graphics and figure styling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 0.24.2.\n"
     ]
    }
   ],
   "source": [
    "#to check version of scikit-learn\n",
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common terminology:\n",
    " - **Cross-validation** is a technique of dividing the data into *training data* (used to train the algorithm) and *testing data* (used to test/validate the trained algorithm). The goal is to measure how well the model predicts new input, and to limit problems of [overfitting](https://en.wikipedia.org/wiki/Overfitting). See flowchart [here](https://scikit-learn.org/stable/modules/cross_validation.html).  One popular technique is k-fold cross-validation with k-group of datasets.  See an example [here](https://en.wikipedia.org/wiki/Cross-validation_\\(statistics\\)). \n",
    "\n",
    " - **Feature extraction** involves creating suitable feature variables and the corresponding feature matrices that can be passed to a machine learning algorithm. The module `sklearn.feature_extraction` has methods that automatically assemble feature matrices (similar to `Patsy` formula library in statistical models). \n",
    " \n",
    " - **Dimensionality reduction** and **feature selection** eliminate less useful features and therefore reduce the dimensionality of the problem. This is important when the number of features is larger than or comparable to the number of observations. The modules `sklearn.decomposition` and `sklearn.feature_selection` contain method for reducing the dimensionality. One technique is principle component analysis (PCA) which computes the singular value decomposition of the feature matrix and keeps only most important singular vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with built-in datasets in `Scikit-Learn`. The `datasets` module in `sklearn` has 3 groups of functions for:\n",
    "1. loading built-in datasets:  with prefix `load_`, e.g., `load_boston`\n",
    "2. fetching external datasets: with prefix `fetch_`, e.g., `fetch_califonia_housing`\n",
    "3. generating random datasets from random numbers: with prefix `make_`, e.g., `make_regression`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "Consider a problem with 50 samples and 50 features out of which only 10 features are informative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed so that we get the same random numbers\n",
    "np.random.seed(123)\n",
    "# generate random dataset \n",
    "X_all, y_all = datasets.make_regression(n_samples=50, n_features=50, n_informative=10, noise=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_all)\n",
    "#print(X_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into 2 equal size datasets: a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "   model_selection.train_test_split(X_all, y_all, train_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Ordinary Least Squares:  \n",
    "minimize the residual sum of squares between the observed responses in the dataset, and the responses predicted by the linear approximation \n",
    "[documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of ordinary linear regression model in sklearn.linear_model module\n",
    "model = linear_model.LinearRegression()\n",
    "# Fit the model to the data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our regression problem has 50 features but only 25 samples were used to train the model. So we expect overfiting. Let's quantify this in a few ways:\n",
    "1) Compute the sum of squared errors (SSE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to compute SSE\n",
    "def sse(resid):\n",
    "    return np.sum(resid**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute SSE for our training dataset\n",
    "resid_train = y_train - model.predict(X_train)\n",
    "sse_train = sse(resid_train)\n",
    "sse_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute SSE for our test dataset\n",
    "resid_test = y_test - model.predict(X_test)\n",
    "sse_test = sse(resid_test)\n",
    "sse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Compute the r-squared score using the `score` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Plot the residuals of training and testing datasets and look at the values of coefficients computed by the residual.  We can extract the fitted parameters by the `coef_` attribute of a `LinearRegression` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for plotting the residuals and coefficients \n",
    "def plot_residuals_and_coeff(resid_train, resid_test, coeff):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "    axes[0].bar(np.arange(len(resid_train)), resid_train)\n",
    "    axes[0].set_xlabel(\"sample number\")\n",
    "    axes[0].set_ylabel(\"residual\")\n",
    "    axes[0].set_title(\"training data\")\n",
    "    axes[1].bar(np.arange(len(resid_test)), resid_test)\n",
    "    axes[1].set_xlabel(\"sample number\")\n",
    "    axes[1].set_ylabel(\"residual\")\n",
    "    axes[1].set_title(\"testing data\")\n",
    "    axes[2].bar(np.arange(len(coeff)), coeff)\n",
    "    axes[2].set_xlabel(\"coefficient number\")\n",
    "    axes[2].set_ylabel(\"coefficient\")\n",
    "    fig.tight_layout()\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function \n",
    "fig, ax = plot_residuals_and_coeff(resid_train, resid_test, model.coef_)\n",
    "\n",
    "# note the scale on the residual axis for the training data plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why we have overfitting in this example is that there are too few samples. A solution could be to get more samples but this is not always practical (e.g. too expensive to collect more data).  \n",
    "\n",
    "* This is related to the ill-conditioning problem we saw in the numerical section of class when trying to fit data with high-order polynomials.  The matrices became nearly singular and the resulting fit was prone to oscillating between the fitting points and was also very sensitive to noise.\n",
    "* Making an ill-conditioned problem better behaved is call *regularization*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized regression\n",
    "Let \n",
    "* $X$ be the feature matrix (i.e, independent variables), \n",
    "* $y$ be the response variables (i.e., the observed values), and \n",
    "* $\\beta$ the vector of model parameters.\n",
    "\n",
    "Ordinary linear regression varies the model parameters ($\\beta$) to minimize the sum of the squares of the errors \n",
    "- $~\\min_{\\beta} \\| X\\beta - y \\|_2^2  $\n",
    "\n",
    "But this is prone to overfitting especially if there is too little data.  Symptom of overfitting are large parameter values, or parameters that are expected to be unimportant having larger than exepected values.  Thus, regularization adds a penalty term that favors smaller values of the parameters. \n",
    "* [Regularized regression](https://en.wikipedia.org/wiki/Regularized_least_squares):  $~~~~~~\\min_{\\beta} \\|X\\beta - y \\|_2^2 + \\textit{penalty term}  $ \n",
    "* By using a larger penalty term we can more heavily favor smaller values of the parameters at the expense of potentially making the fit worse.\n",
    "\n",
    "We will examine two forms\n",
    "* [LASSO regression](https://en.wikipedia.org/wiki/Lasso_(statistics)):    $~~~~~\\min_{\\beta} \\{ \\|X\\beta - y \\|_2^2 + \\alpha \\| \\beta \\|_1 \\} $  favors a parameter vector with as few nonzero elements as possible  \n",
    "*[Ridge regression](https://en.wikipedia.org/wiki/Regularized_least_squares#Ridge_regression_(or_Tikhonov_regularization)):    $~~~~~~~\\min_{\\beta} \\{ \\|X\\beta - y \\|_2^2 + \\alpha \\| \\beta \\|_2^2 \\} $ favors parameters with smaller coefficients\n",
    "\n",
    "Here $\\alpha$ is the strength of the regularization (free parameter).\n",
    "\n",
    "[Vector norm](https://en.wikipedia.org/wiki/Norm_(mathematics)) reminder\n",
    "* 2-norm or Euclidean norm --- $\\|x\\|_2 = \\sum_{i=1}^{N} x_i^2$\n",
    "* 1-norm or Manhattan norm --- $\\|x\\|_1 = \\sum_{i=1}^{N} |x_i|$\n",
    "\n",
    "\n",
    "Note: \n",
    "* Use [LASSO](https://en.wikipedia.org/wiki/Lasso_(statistics)) regression when you wish to eliminate as many features as possible. \n",
    "* Use [Ridge](https://en.wikipedia.org/wiki/Regularized_least_squares#Ridge_regression_(or_Tikhonov_regularization)) regression when you want to limit the magnitude of coefficients.\n",
    "\n",
    "Also, the two models treat outliers differently\n",
    "* Ridge squares the residual which means it much more heavily penalizes outliers with residual $>1$ compared with LASSO --- so the values of the coefficients fitted by Ridge are impacted more by outliers.\n",
    "* LASSO does not square the residual so even residuals $<1$ might still contribute to the penalty --- so the values of the coefficients fitted by LASSO might be more evenly impacted even by points with smaller residual.\n",
    "\n",
    "\n",
    "More details [here](http://statweb.stanford.edu/~tibs/sta305files/Rudyregularization.pdf) and [here](https://www.stat.cmu.edu/~ryantibs/datamining/lectures/16-modr1.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2a) Ridge regression:  impose a penalty on the size of coefficients  \n",
    " [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge) for Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Ridge regression with alpha = 2.5 \n",
    "model = linear_model.Ridge(alpha=2.5)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute SSE for our training dataset\n",
    "resid_train = y_train - model.predict(X_train)\n",
    "sse_train = sse(resid_train)\n",
    "sse_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute SSE for our test dataset\n",
    "resid_test = y_test - model.predict(X_test)\n",
    "sse_test = sse(resid_test)\n",
    "sse_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute model score\n",
    "model.score(X_train, y_train), model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the residuals and coefficients \n",
    "fig, ax = plot_residuals_and_coeff(resid_train, resid_test, model.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the errors on the training data become *much* larger (because we introduced the penalty term), but the error on the testing data did not change much and similarly the size of the coefficients did not change much either.\n",
    "* Repeat with a larger penalty term to force the coefficients to become smaller\n",
    "\n",
    "For this example the 2-norm penalty term (ridge regression) does not help much because of the way we generated the data --- we know that there should only be 10 explantory variables so rather than trying to make all of the coefficients small we want to try to concentrate the model into as few terms as possible.  \n",
    "* Other data sets can show different behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2b) LASSO regression:  eliminate as many features as possible\n",
    "[Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso) for LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform LASSO regression with alpha = 1.0\n",
    "model = linear_model.Lasso(alpha=1.0)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_train = y_train - model.predict(X_train)\n",
    "sse_train = sse(resid_train)\n",
    "sse_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_test = y_test - model.predict(X_test)\n",
    "sse_test = sse(resid_test)\n",
    "sse_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute model score\n",
    "model.score(X_train, y_train), model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_residuals_and_coeff(resid_train, resid_test, model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients from LASSO regression are mostly zeros. This agrees with our data because when we generated data, we set that only 10 features are informative.  Therefore, if you suspect that many features might not be important in the model, the L1 regularization of LASSO regression is a good method to try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3)  A systematic way of choosing $\\alpha$ \n",
    "The most suitable $\\alpha$ is problem dependent. \n",
    "First, let's look at how the model coefficients and the SSE depends on $\\alpha$ for this specific problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(-4, 2, 100)  #100 numbers (from 10^(-4) to 10^2) spaced evenly on a log scale\n",
    "coeffs = np.zeros((len(alphas), X_train.shape[1])) # size = 100x50\n",
    "sse_train = np.zeros_like(alphas)  #note: np.zeros_like returns an array of zeros with the same shape and type as a given array\n",
    "sse_test = np.zeros_like(alphas)\n",
    "# loop through the alpha values and perform LASSO regression for each alpha\n",
    "for n, alpha in enumerate(alphas):\n",
    "    model = linear_model.Lasso(alpha=alpha, max_iter=10000)\n",
    "    model.fit(X_train, y_train)\n",
    "    coeffs[n, :] = model.coef_\n",
    "    resid = y_train - model.predict(X_train)\n",
    "    sse_train[n] = sum(resid**2)\n",
    "    resid = y_test - model.predict(X_test)\n",
    "    sse_test[n] = sum(resid**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the coefficients and SSE \n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharex=True)\n",
    "\n",
    "for n in range(coeffs.shape[1]):\n",
    "    axes[0].plot(np.log10(alphas), coeffs[:, n], color='k', lw=0.5)\n",
    "\n",
    "axes[1].semilogy(np.log10(alphas), sse_train, label=\"train\")\n",
    "axes[1].semilogy(np.log10(alphas), sse_test, label=\"test\")\n",
    "axes[1].legend(loc=0)\n",
    "\n",
    "axes[0].set_xlabel(r\"${\\log_{10}}\\alpha$\", fontsize=18)\n",
    "axes[0].set_ylabel(r\"coefficients\", fontsize=18)\n",
    "axes[1].set_xlabel(r\"${\\log_{10}}\\alpha$\", fontsize=18)\n",
    "axes[1].set_ylabel(r\"sse\", fontsize=18)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:*\n",
    "- Left plot: When $\\alpha$ is very small, many coefficients are nonzero. When $\\alpha$ is increased above a certain value, many of the coefficients collapse to 0 while only a few remain nonzero. \n",
    "- Right plot: While the SSE for the training dataset is steadily increasing, the SSE of the testing dataset has a sharp drop when $\\log_{10} \\alpha$ is around -2 to -1 and then slowly increases. When $\\alpha$ is too large ($\\log_{10} \\alpha > 1$), both SSEs are very large.\n",
    "\n",
    "It is *very tempting* to just do what we just did --- adjust $\\alpha$ to give the best error on our test data set but this is **very wrong**.  \n",
    "* We just fitted our test data!\n",
    "* So how can we now test the quality of the fit? You cannot!\n",
    "\n",
    "$\\alpha$ is an example of a hyperparameter\n",
    "* You can think of it as a knob that you can adjust to control the behavior of the underlying model that has parameters $\\beta$ obtained by fitting the training data.\n",
    "* But how to optimize $\\alpha$ if we cannot use our test data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn can automatically perform a search for the optimal $\\alpha$ with the `LassoCV` and `RidgeCV` classes. This involves a [cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html) method ($k$-fold cross-validation: $k=3$ by default now or use the `cv` argument).\n",
    "\n",
    "Follow the link and read about CV and why it helps especially with small data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Lasso method with automatically choosen alpha\n",
    "model = linear_model.LassoCV(cv=3)\n",
    "model.fit(X_all, y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can get the alpha selected by the `.alpha_` attribute\n",
    "model.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the SSE of the training set \n",
    "resid_train = y_train - model.predict(X_train)\n",
    "sse_train = sse(resid_train)\n",
    "sse_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the SSE of the test set \n",
    "resid_test = y_test - model.predict(X_test)\n",
    "sse_test = sse(resid_test)\n",
    "sse_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_train, y_train), model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the residual and the SSE\n",
    "fig, ax = plot_residuals_and_coeff(resid_train, resid_test, model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) A combination of  LASSO and Ridge regressions (elastic net regression)\n",
    "The objective function is\n",
    "$$\n",
    "     \\min_{\\beta} \\{ \\|X\\beta - y \\|_2^2 + \\alpha \\rho \\| \\beta \\|_1 + \\alpha (1 - \\rho) \\| \\beta \\|_2^2 \\} \n",
    "$$\n",
    "where $\\rho$ determines the relative weight of the L1 and L2 penalties. In Scikit-learn, we use the [`ElasticNet`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNetCV.html?highlight=elasticnetcv#sklearn.linear_model.ElasticNetCV) class with `l1_ratio` as the $\\rho$ parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NOTE: here we use rho = 1/2 (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_model.ElasticNetCV(cv=3,l1_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.l1_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resid_train = y_train - model.predict(X_train)\n",
    "sse_train = sse(resid_train)\n",
    "sse_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_test = y_test - model.predict(X_test)\n",
    "sse_test = sum(resid_test**2)\n",
    "sse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_train, y_train), model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_residuals_and_coeff(resid_train, resid_test, model.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: recall that we used $\\rho$ = 1/2 so we expected to see the characteristics of both LASSO(making coefficients 0) and Ridge (suppressing the size of the coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References: \n",
    "- *Numerical Python: A Practical Techniques Approach for Industry*  by Robert Johansson (Chapter 15)\n",
    "- *Python Data Science Handbook* by Jake VanderPlas\n",
    "- https://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
