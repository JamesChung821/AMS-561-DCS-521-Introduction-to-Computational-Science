{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning: Classification & Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import model_selection  \n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn import neighbors\n",
    "from sklearn import svm\n",
    "from sklearn import ensemble\n",
    "from sklearn import cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed so that we get the same random numbers\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 0.24.2.\n"
     ]
    }
   ],
   "source": [
    "# check version of scikit-learn\n",
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "Recall that we already saw an example of classification where we used a logistic regression to classify Iris flower samples from sepal and petal widths and lengths.  Let's consider this data again but now we will conder all 3 species.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load built-in data in the Scikit-Learn library \n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bunch is a dictionary-like object. Some useful attributes are:\n",
    "- `data`: the data to learn\n",
    "- `target`: the classification labels\n",
    "- `target_names`: the meaning of the labels\n",
    "- `feature_names`: the meaning of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into a training part(70%) and a testing part (30%)\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(iris.data, iris.target, train_size = 0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step: create a classifer instance \n",
    "classifier = linear_model.LogisticRegression(solver=\"lbfgs\",multi_class=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the 'fit' method to train the classifer\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class for the samples in the testing datasets\n",
    "#    (so that we can compare the predictions with the actual values)\n",
    "y_test_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, 1, 0, 1, 1, 0, 0, 1, 2, 0, 1, 2, 2, 2, 0, 0, 1, 0, 0, 1,\n",
       "       0, 2, 0, 0, 0, 2, 2, 0, 2, 1, 0, 0, 1, 1, 2, 0, 0, 1, 1, 0, 2, 2,\n",
       "       2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sklearn.metrics.classification_report` function returns a text report showing the main classification metrics (see detail [here](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support) )\n",
    "\n",
    "Also:\n",
    "* [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall)\n",
    "* [F1 score](https://en.wikipedia.org/wiki/F1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the *confusion matrix* $C$ where $C_{ij}$ is the number of samples of category $i$ that were categorized as *j*.  That is, the diagonal elements corresponds to the number of correctly classified samples for each category, and the off-diagonal elements are the number of incorrectly classified samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(y_test, y_test_pred)\n",
    "## NOTE1:  the numbers depend on the random seed\n",
    "## NOTE2:  the sum of each row is the total number of samples for the corresponding category. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count unique value in y_test\n",
    "np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also plot the confusion matrix\n",
    "# Note: more recent sklearn provides a function for this \n",
    "metrics.plot_confusion_matrix(classifier, X_train, y_train)\n",
    "\n",
    "# In case your version does not have it\n",
    "#sn.set(font_scale=1.4) # for label size\n",
    "#sn.heatmap(metrics.confusion_matrix(y_test, y_test_pred), \n",
    "#           annot=True, annot_kws={\"size\": 16}) # font size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have just used a logistic regression model as a classifier. Other popular classifiers are \n",
    "- decision trees  ([detail](https://en.wikipedia.org/wiki/Decision_tree_learning))\n",
    "- nearest neighbor methods ([detail](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) and [overview](https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn))\n",
    "- support-vector machine (SVM) ([detail](https://en.wikipedia.org/wiki/Support_vector_machine ))\n",
    "- Random Forest method ([detail](https://en.wikipedia.org/wiki/Random_forest) )\n",
    "\n",
    "See a [flowchart](https://scikit-learn.org/stable/tutorial/machine_learning_map/) of a rough guide on how to choose an estimator and detailed [comparison](https://www.dataschool.io/comparing-supervised-learning-algorithms/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a decision tree as classifier\n",
    "classifier = tree.DecisionTreeClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_test_pred = classifier.predict(X_test)\n",
    "metrics.confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the decision tree\n",
    "tree.plot_tree(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the nearest neighbor classifier\n",
    "classifier = neighbors.KNeighborsClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_test_pred = classifier.predict(X_test)\n",
    "metrics.confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SVM classifer --- next class is devoted to SVM\n",
    "classifier = svm.SVC(gamma=\"auto\")\n",
    "classifier.fit(X_train, y_train)\n",
    "y_test_pred = classifier.predict(X_test)\n",
    "metrics.confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Random Forest classifer\n",
    "classifier = ensemble.RandomForestClassifier(n_estimators=100)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_test_pred = classifier.predict(X_test)\n",
    "metrics.confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the feature importances --- we again see the petal width/height are most informative\n",
    "classifier.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will vary the training and testing sample sizes and compare four classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a numpy array with training size ratios, ranging from 10% to 90%\n",
    "train_size_vec = np.linspace(0.1, 0.9, 30)\n",
    "\n",
    "# create a list of classifiers\n",
    "classifiers = [tree.DecisionTreeClassifier,\n",
    "               neighbors.KNeighborsClassifier,\n",
    "               svm.SVC,\n",
    "               ensemble.RandomForestClassifier\n",
    "              ]\n",
    "\n",
    "# create an array that stores the diagonals of the confusion matrix as a function of training size ratio\n",
    "# and classifier\n",
    "cm_diags = np.zeros((3, len(train_size_vec), len(classifiers)), dtype=float)\n",
    "\n",
    "# loop over each training size ratio and classifier\n",
    "for n, train_size in enumerate(train_size_vec):\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        model_selection.train_test_split(iris.data, iris.target, train_size=train_size)\n",
    "\n",
    "    for m, Classifier in enumerate(classifiers): \n",
    "        classifier = Classifier()\n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_test_pred = classifier.predict(X_test)\n",
    "        cm_diags[:, n, m] = metrics.confusion_matrix(y_test, y_test_pred).diagonal()\n",
    "        cm_diags[:, n, m] /= np.bincount(y_test)\n",
    "        \n",
    "# plot accuracy as a function of training size ratio\n",
    "fig, axes = plt.subplots(1, len(classifiers), figsize=(12, 3))\n",
    "\n",
    "for m, Classifier in enumerate(classifiers): \n",
    "    axes[m].plot(train_size_vec, cm_diags[2, :, m], label=iris.target_names[2])\n",
    "    axes[m].plot(train_size_vec, cm_diags[1, :, m], label=iris.target_names[1])\n",
    "    axes[m].plot(train_size_vec, cm_diags[0, :, m], label=iris.target_names[0])\n",
    "    axes[m].set_title(type(Classifier()).__name__,fontsize=\"x-small\")\n",
    "    axes[m].set_ylim(0, 1.1)\n",
    "    axes[m].set_xlim(0.1, 0.9)\n",
    "    axes[m].set_ylabel(\"classification accuracy\",fontsize=\"x-small\")\n",
    "    axes[m].set_xlabel(\"training size ratio\",fontsize=\"x-small\")\n",
    "    axes[m].legend(loc=4,fontsize=\"x-small\")\n",
    "    axes[m].tick_params(axis=\"x\", labelsize=12)\n",
    "    axes[m].tick_params(axis=\"y\", labelsize=12)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that the error is different for each model.\n",
    "- Which classifier is best depends on the problem.\n",
    "- The good news is that it's easy to switch them in Scikit-learn.\n",
    "- Besides accuracy, computational performance can be important.  For large problems with many features, a decision tree method such as Random Forest is a good one to try first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "- Clustering can be considered as a classification problem where the classes are NOT known. For more details, see [Wikipedia](https://en.wikipedia.org/wiki/Cluster_analysis)).\n",
    "- It is an example of unsupervised learning (data is unlabeled). \n",
    "- The input of a clustering algorithm contains only the feature variables and the output of the algorithm is an array of integers that represent a cluster(or class) of each sample.\n",
    "- Popular clustering methods are:\n",
    "    - [*K-means algorithm*](https://en.wikipedia.org/wiki/K-means_clustering): groups the samples into clusters such that the within-group sum of square deviation is minimized.  ( `sklearn.cluster.Kmeans`)\n",
    "    - [*mean-shift algorithm*](https://en.wikipedia.org/wiki/Mean_shift) : clusters the samples by fitting the data to density functions (e.g. Gaussian functions)  ( `sklearn.cluster.MeanShift`)\n",
    "\n",
    " A full list of methods in Scikit-Learn [here](http://scikit-learn.org/stable/modules/clustering.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** Consider again the Iris dataset but this time we will not use the response variable. We will implement the K-means method. We need to specify the number of clusters (we will use `n_clusters = 3` since we know this in advance). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store feature data in X and response data in y\n",
    "X, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed so that we get the same random numbers\n",
    "np.random.seed(555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1: create an instance of KMeans class using number of clusters = 3\n",
    "clustering = cluster.KMeans(n_clusters = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2: call the fit() method\n",
    "clustering.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step3: use predict() method to make prediction\n",
    "y_pred = clustering.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the output is long, we'll look at every 8th element\n",
    "y_pred[::8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[::8]\n",
    "## NOTE: there is a good correlation btw the two, but the output has assigned different numbers to the groups\n",
    "##   than what was used in the target vector\n",
    "## - To be able to compare two arrays with metrics such as the confusion matrix, we need to rename the elements\n",
    "##      so that the same integers are used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the elements in y_pred so that the same integers are used as in y\n",
    "idx_0, idx_1, idx_2 = (np.where(y_pred == n) for n in range(3))\n",
    "y_pred[idx_0], y_pred[idx_1], y_pred[idx_2] = 0, 2, 1\n",
    "y_pred[::8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the confusion matrix\n",
    "metrics.confusion_matrix(y, y_pred)\n",
    "\n",
    "## NOTE(numbers might be different): the algorithm was able to correctly identify all samples in group 0 (first species) as a group of its own.\n",
    "#  2 elements from group 1 was assigned to group 2\n",
    "# 14 elements from group 2 was assinged to group 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make scatter plots for each pair of features\n",
    "\n",
    "N = X.shape[1]\n",
    "\n",
    "fig, axes = plt.subplots(N, N, figsize=(12, 12), sharex=True, sharey=True)\n",
    "\n",
    "colors = [\"coral\", \"blue\", \"green\"] # different color for each cluster\n",
    "markers = [\"^\", \"v\", \"o\"]           # different symbol for each cluster\n",
    "n_clusters = 3\n",
    "for m in range(N):\n",
    "    for n in range(N):\n",
    "        for p in range(n_clusters):\n",
    "            mask = y_pred == p\n",
    "            axes[m, n].scatter(X[:, m][mask], X[:, n][mask],\n",
    "                               marker=markers[p], s=30, \n",
    "                               color=colors[p], alpha=0.25) # alpha is transparency\n",
    "\n",
    "        for idx in np.where(y != y_pred):  # Put a red rectangle at bad predictions\n",
    "            axes[m, n].scatter(X[idx, m], X[idx, n],\n",
    "                               marker=\"s\", s=30, \n",
    "                               edgecolor=\"red\", \n",
    "                               facecolor=(1,1,1,0))\n",
    "        axes[m,n].set_xlim([0,8])\n",
    "        axes[m,n].set_ylim([0,8])\n",
    "        axes[m,n].set_xticks([0,2,4,6,8])\n",
    "        axes[m,n].set_yticks([0,2,4,6,8])\n",
    "\n",
    "    axes[N-1, m].set_xlabel(iris.feature_names[m], fontsize=16)\n",
    "    axes[m, 0].set_ylabel(iris.feature_names[m], fontsize=16)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"clustering_iris.pdf\")\n",
    "## NOTE: the clustering does a very good job at recognizing which sample belongs to distinct group,\n",
    "## but because of the overlap in the features we cannot expect any unsupervised clustering algorithm can\n",
    "## fully resolve the various groups in the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References: \n",
    "- *Numerical Python: A Practical Techniques Approach for Industry*  by Robert Johansson (Chapter 15)\n",
    "- *Python Data Science Handbook* by Jake VanderPlas\n",
    "- https://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
