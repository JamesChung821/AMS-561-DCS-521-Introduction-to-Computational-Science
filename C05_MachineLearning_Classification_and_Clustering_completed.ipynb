{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning: Classification & Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import model_selection  \n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn import neighbors\n",
    "from sklearn import svm\n",
    "from sklearn import ensemble\n",
    "from sklearn import cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed so that we get the same random numbers\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 0.24.2.\n"
     ]
    }
   ],
   "source": [
    "# check version of scikit-learn\n",
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "Recall that we already saw an example of classification where we used a logistic regression to classify Iris flower samples from sepal and petal widths and lengths.  Let's consider this data again but now we will conder all 3 species.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load built-in data in the Scikit-Learn library \n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bunch is a dictionary-like object. Some useful attributes are:\n",
    "- `data`: the data to learn\n",
    "- `target`: the classification labels\n",
    "- `target_names`: the meaning of the labels\n",
    "- `feature_names`: the meaning of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into a training part(70%) and a testing part (30%)\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(iris.data, iris.target, train_size = 0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step: create a classifer instance \n",
    "classifier = linear_model.LogisticRegression(solver=\"lbfgs\",multi_class=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the 'fit' method to train the classifer\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class for the samples in the testing datasets\n",
    "#    (so that we can compare the predictions with the actual values)\n",
    "y_test_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, 1, 0, 1, 1, 0, 0, 1, 2, 0, 1, 2, 2, 2, 0, 0, 1, 0, 0, 1,\n",
       "       0, 2, 0, 0, 0, 2, 2, 0, 2, 1, 0, 0, 1, 1, 2, 0, 0, 1, 1, 0, 2, 2,\n",
       "       2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, 1, 0, 2, 1, 0, 0, 1, 2, 0, 1, 2, 2, 2, 0, 0, 1, 0, 0, 2,\n",
       "       0, 2, 0, 0, 0, 2, 2, 0, 2, 2, 0, 0, 1, 1, 2, 0, 0, 1, 1, 0, 2, 2,\n",
       "       2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sklearn.metrics.classification_report` function returns a text report showing the main classification metrics (see detail [here](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support) )\n",
    "\n",
    "Also:\n",
    "* [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall)\n",
    "* [F1 score](https://en.wikipedia.org/wiki/F1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        18\n",
      "           1       0.77      1.00      0.87        10\n",
      "           2       1.00      0.82      0.90        17\n",
      "\n",
      "    accuracy                           0.93        45\n",
      "   macro avg       0.92      0.94      0.92        45\n",
      "weighted avg       0.95      0.93      0.93        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the *confusion matrix* $C$ where $C_{ij}$ is the number of samples of category $i$ that were categorized as *j*.  That is, the diagonal elements corresponds to the number of correctly classified samples for each category, and the off-diagonal elements are the number of incorrectly classified samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18,  0,  0],\n",
       "       [ 0, 10,  0],\n",
       "       [ 0,  3, 14]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, y_test_pred)\n",
    "## NOTE1:  the numbers depend on the random seed\n",
    "## NOTE2:  the sum of each row is the total number of samples for the corresponding category. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 10, 17], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count unique value in y_test\n",
    "np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x176d057ec40>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATIAAAEGCAYAAADmLRl+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAbzUlEQVR4nO3de5RcZZnv8e+vO50rCSR0CCEGCRqDkVtYGUzE4QQUCcycQebo8cLisEYU4cCoMzpLjni8McPROSCjgGgUluhwMQg54BViIAM4gLkYICGEa7glQDqQC4Qk3VXP+aN2Q5N0ump3V/Xeu/v3WWuv1N5V9e6HIjy877vfiyICM7Mia8o6ADOzvnIiM7PCcyIzs8JzIjOzwnMiM7PCG5J1AF2NGjs0xk4akXUYubVlVXPWIVjBbec1dsYO9aWME48bFRtfLtX02WUP7rgtIub25X61yFUiGztpBH8/f3bWYeTWHw4dnXUIVnD3x6I+l9H2con7b3tbTZ9tmfhEa59vWINcJTIzK4KgFOWsg3gLJzIzSyWAMvkaSO9EZmaplXGNzMwKLAja3bQ0syILoOSmpZkVnfvIzKzQAijlbNUcJzIzSy1fPWROZGaWUhDuIzOzYouA9nzlMScyM0tLlOjTdM26cyIzs1QCKLtGZmZF5xqZmRVaZUCsE5mZFVgA7ZGvNVmdyMwslUCUcra4tBOZmaVWDjctzazA8thHlq/6oZkVgChFU01Hj6VIwyX9SdIDklZJ+mZy/RuSnpe0IjlOrhaRa2Rmlkplhdi61IF2AMdHxKuSWoB7JP0uee/SiLi41oKcyMwslQixM/q+o1dEBPBqctqSHL0aauumpZmlVkY1HUCrpKVdjrO6liOpWdIK4CVgYUTcn7x1nqQHJV0taWy1eFwjM7NUKp39NdeB2iJi5h7LiigBR0raB1gg6VDgSuDC5FYXApcAn+rpJq6RmVlK9ens7yoiNgGLgbkR8WJElCKiDPwYOLra953IzCyVzs7+Wo6eSBqf1MSQNAL4IPCIpIldPnYqsLJaTG5amllqpfoMiJ0IXCOpmUqlan5E/FrSzyUdSSVnrgU+W60gJzIzSyUQ7dH31BERDwIzurl+etqynMjMLJWUnf39wonMzFIJVK+mZd04kZlZanUa2V83TmRAaQcsO2Mk5Z0QJdjvhA7ecd5OHrt4GBv+o5mmITBicpnp/7ydljFZR5sPM+ds4ewL19HcFPzu+nHMv3xC1iHlykD+fSJINbSiPzQ0GklzJa2R9Lik8xt5r75oGgpHXb2NWTdv472/3MbGPw5h8wNNjJvdwawF25i1YBsjDyqz9idDsw41F5qagnMvep6vnjaFz8yZxnGnbOLAqduzDis3BvrvU+nsb67p6C8NS2TJI9UrgJOA6cAnJE1v1P36QoIhIyuvo6NyINj3mBJNSZ1178PL7HgxX/8Xysq0GdtYt3YoLzwzjI72Jhbfsg+zT9ycdVi5MRh+nxJNNR39pZF3Ohp4PCKejIidwA3AKQ28X59ECe77byO569i9GDe7g70Pf+teyusWtLDv+zsyii5f9t2/nQ3r3qydtq1voXVie4YR5ctA/30CUY7ajv7SyEQ2CXi2y/lzybVcUjPMumkb71/0KlseaubVx978aZ760VDUHOz/105kUKnB7ipytj1YlgbD7zOYamTdpePd/nVKOqtzZvxrL+9sYDi1aRkDY/+ixMZ7Ku37dbcMoe2uIRz6ne3d/gUdjNrWtzD+gDf/XbVObGfjCy0ZRpQvA/33qexr2VTT0V8aeafngMldzt8GrNv1QxExLyJmRsTMUeOy6Uzf+bJo31J5XdoOL9/XzMgpZdruaebpq4ZyxGWv0zwik9Byac2KkUyaspMJk3cwpKXMnFM2cd/te2cdVm4M/N+nstN4LUd/aeTwiyXAVElTgOeBjwOfbOD9em3HBrHqghFQqjQBJpzYwfg5Jf540ijKO2H5ZypZbO/DS7z76zsyjjZ75ZK44oJJXHTdkzQ1w+03jOPpR4dnHVZuDPTfp7IdXP89kaxFwxJZRHRIOg+4DWgGro6IVY26X1+MnlZm1i+37Xb9mN+9lkE0xbDkjjEsucOD6vZkIP8+EerXZmMtGjogNiJ+C/y2kfcws/6XtwGxHtlvZqlU1iPL15MvJzIzS0mukZlZsVWGX7hGZmYF1jnXMk+cyMwstbwt45OvaMws9yrL+KimoyeShkv6k6QHJK2S9M3k+jhJCyU9lvxZdV9LJzIzS61Ok8Z3AMdHxBHAkcBcSbOA84FFETEVWJSc98iJzMxSqax+0fe5llHxanLakhxBZZWca5Lr1wAfrhaT+8jMLJXKFKX61IGSdQuXAe8EroiI+yVNiIj1ABGxXtJ+1cpxIjOzlFJNUWqVtLTL+byImNd5EhEl4Mhko94Fkg7tTUROZGaWWoqR/W0RMbPahyJik6TFwFzgRUkTk9rYROClat93H5mZpVLHp5bjk5oYkkYAHwQeAW4Fzkg+dgZwS7WYXCMzs9TqtPrFROCapJ+sCZgfEb+WdC8wX9KZwDPAR6sV5ERmZql0rtnf53IiHgRmdHN9I/CBNGU5kZlZKgF0eNK4mRXdoFpY0cwGoH7e6q0WTmRmlooXVjSzAcE1MjMrNC+saGaFF4iOsjv7zazg3EdmZsUWblqaWcG5j8zMBgQnMjMrtECU3NlvZkXnzn4zK7RwZ7+ZDQThRGZmxeZJ42Y2ALhG1oMtq5r5w6Gjsw4jt25btyLrEHLvpINnZR1Crml7PVZ2hVLZiczMCi5vTy3zNRjEzHIvqDQtazl6ImmypDslrZa0StLnk+vfkPS8pBXJcXK1mFwjM7OU6tbZ3wF8MSKWSxoNLJO0MHnv0oi4uNaCnMjMLLWIepQR64H1yeutklYDk3pTlpuWZpZaiqZlq6SlXY6zuitP0kFUtoa7P7l0nqQHJV0taWy1eFwjM7NUKk8ta64DtUXEzJ4+IGkv4CbgCxGxRdKVwIVUuuMuBC4BPtVTGU5kZpZaPZqWAJJaqCSxayPi5krZ8WKX938M/LpaOW5amllqdXpqKeAqYHVEfLfL9YldPnYqsLJaPK6RmVkqQfUkVaNjgNOBhyStSK59BfiEpCOpNC3XAp+tVpATmZmlVo+WZUTcA92OrP1t2rKcyMwsnYDwFCUzKzpPGjezwqvXU8t62WMik3QZPTSFI+JzDYnIzHKtc65lnvRUI1vab1GYWXEEUJREFhHXdD2XNCoiXmt8SGaWd3lrWlYdECtptqSHgdXJ+RGSftDwyMwsp0SUazv6Sy0j+/8NOBHYCBARDwDHNjAmM8u7qPHoJzU9tYyIZyuzCd5Qakw4ZpZ7UazO/k7PSnofEJKGAp8jaWaa2SBVtD4y4GzgXCoLnj0PHJmcm9mgpRqP/lG1RhYRbcBp/RCLmRVFOesA3qqWp5YHS/qVpA2SXpJ0i6SD+yM4M8uhznFktRz9pJam5XXAfGAicABwI3B9I4Mys3yLqO3oL7UkMkXEzyOiIzn+ndx19ZlZvyrK8AtJ45KXd0o6H7iBSmgfA37TD7GZWV4VaPjFMiqJqzPirqs0dm4KYGaDkHLWJutpruWU/gzEzAoiBEVcWFHSocB0YHjntYj4WaOCMrOcy1mNrJbhF18HLkuO44B/Bf6mwXGZWZ7VobNf0mRJd0paLWmVpM8n18dJWijpseTPqhv01vLU8iPAB4AXIuLvgCOAYTV8z8wGqvo8tewAvhgR7wZmAedKmg6cDyyKiKnAouS8R7U0LV+PiLKkDkljgJeAAT0gduacLZx94Tqam4LfXT+O+ZdPyDqkzO3cLr74t++kfWcTpQ74y7/azP/4pxd4YtVwLjt/Mq+/1sSEt+3ky1c8zajRORv2nYF/+M6THH3cK2za2MI5Jx2edTj1VaeFFSNiPbA+eb1V0moqUyFPAeYkH7sGWAx8uaeyaqmRLZW0D/BjKk8ylwN/qvYlSVcnMwGqbq6ZJ01NwbkXPc9XT5vCZ+ZM47hTNnHg1O1Zh5W5lmHBv974BD/8wxquXLiGpYtHs3rZSP7tSwfyqa+s40d3rOGYkzbzyyv3yzrUXFj4y1a++neHZB1GwyhqO4BWSUu7HGd1W550EDADuB+YkCS5zmRX9S9V1UQWEf8zIjZFxA+BE4AzkiZmNT8F5tbwuVyZNmMb69YO5YVnhtHR3sTiW/Zh9ombsw4rcxKMGFWpaXW0i1K7kOC5J4Zx2KzKwsEzjt3KPb/ZJ8Mo82PlkjFs3TSA9/apvWnZFhEzuxzzdi1K0l7ATcAXImJLb8LZYyKTdNSuBzAOGJK87lFE3AW83JugsrTv/u1sWDf0jfO29S20TmzPMKL8KJXgnA9O42OHH8qMY7dyyFHbePu07dx72xgA7v71PmxY15JxlNYfUtTIei5HaqGSxK6NiJuTyy9Kmpi8P5FKd1aPevpfxiU9vBfA8dXDrC6pap4FMJyR9SiyT9RN0z9v65NnpbkZrvzDGl7d3Mw3zzyItY8M5x+/+wxX/u9JXHvp/sz+0GaGDPWPNSjUoY9MldVarwJWR8R3u7x1K3AG8O3kz1uqldXTgNjj+hhnTZKq5jyAMRqX+X8FbetbGH/AzjfOWye2s/EF1zK62mvvEkfMfpUld47mo+ds4P/c8CRQaWbev2hMxtFZw9VvHuUxwOnAQ5JWJNe+QiWBzZd0JvAM8NFqBQ3gRnzvrFkxkklTdjJh8g42vtDCnFM28e1z3551WJnbtLGZIUMqSWzH62L53aP57+e+xKa2IezT2kG5DNd9bwJ/ffrGrEO1/lCHRBYR97Dn1Rc/kKYsJ7JdlEviigsmcdF1T9LUDLffMI6nHx1e/YsD3MsvtnDx5w+kXBblMhz7Xzcx64QtLPhJK7/6aSsAx5y0mQ99vHDdog3x5e89zuHv3cKYsR38/I/L+fn33sbt8wfOE13lbIRNwxKZpOupjAVplfQc8PWIuKpR96unJXeMYckdbiJ1dfD07fxg4aO7XT/1022c+um2DCLKt+98/p1Zh9BYmXcCvVXVRJZ0yJ0GHBwR35J0ILB/RPQ4liwiPlGnGM0sR2p9ItmfahkQ+wNgNtCZmLYCVzQsIjPLv5wtdV1L0/K9EXGUpD8DRMQrybZwZjZY5axGVksia5fUTBK6pPHkbg8VM+tPeWta1pLIvg8sAPaT9C9UVsP4akOjMrP8igI+tYyIayUtozKuQ8CHI8I7jZsNZkWrkSVPKbcBv+p6LSKeaWRgZpZjRUtkVHZM6tyEZDgwBVgDvKeBcZlZjhWujywiDut6nqx88dk9fNzMrN+lHtkfEcsl/UUjgjGzgihajUzSP3Y5bQKOAjY0LCIzy7ciPrUERnd53UGlz+ymxoRjZoVQpBpZMhB2r4j4p36Kx8xyThSos1/SkIjoqGVZazMbZIqSyKjslHQUsELSrcCNwGudb3ZZX9vMBpMcrn5RSx/ZOGAjlTX6O8eTBeBEZjZYFaizf7/kieVK3kxgnXKWj82sP+WtRtbTemTNwF7JMbrL687DzAar2ve17FF3G3lL+oak5yWtSI6Tq5XTU41sfUR8q3ooZjao1G8XJahs5H058LNdrl8aERfXWkhPiaz/lnc0s0KpV9MyIu6SdFBfy+mpaZlqOyYzG0Rqb1q2Slra5TirxjucJ+nBpOk5ttqHe9qg1/t6mVm3UkxRaouImSmLvxK4kEoqvBC4BPhUT1+oZfMRM7M31Vob62XzMyJejIhSRJSBHwNHV/uOE5mZpaIUR6/KlyZ2OT2VyhCwHnmncTNLr06d/d1t5A3MkXRkcpe11LD+oROZmaVWx6eW3W3kfVXacpzIzCy9nI3sdyIzs3QKurCimdlbuUZmZkWXt0njTmRmlp4TmfXWyYcdn3UIuTfj3rasQ8i1Bz9Zn84t18jMrNiCQi2saGa2m0JtPmJmtkdOZGZWdIp8ZTInMjNLp74rxNaFE5mZpeY+MjMrPE9RMrPic43MzAqtoDuNm5m9lROZmRWZB8Sa2YCgcr4ymROZmaWTw3Fk3kXJzFJTubajajmVDXhfkrSyy7VxkhZKeiz5s+oGvU5kZpZe/fa1/Ckwd5dr5wOLImIqsCg575ETmZmlpqjtqCYi7gJe3uXyKcA1yetrgA9XK8d9ZGaWTgC1TxpvlbS0y/m8iJhX5TsTImI9QESsl7RftZs4kZlZaimmKLVFxMwGhgK4aWlmKXWOI6tH03IPXpQ0ESD586VqX3AiM7N0Imo/eudW4Izk9RnALdW+4ERmZqnVq0Ym6XrgXmCapOcknQl8GzhB0mPACcl5j9xHZmbp1WlAbER8Yg9vfSBNOU5kZpaa51qaWbEFUMpXJnMiM7PUXCMzs+LzLkpmVnSukZlZseVwGR8nMjNLRYDc2W9mReedxs2s2Ny0LIaZc7Zw9oXraG4Kfnf9OOZfPiHrkHKldcJ2vnjRasa27iTK8PtfHsAt107OOqxMlXfAmjNF7IQowdgPwgHnBK8shHU/FNufgkN+Hox6T9aR1kOf5lE2RMMSmaTJwM+A/YEylXWIvteo+9VLU1Nw7kXP878+fjBt61u47LePcd9te/PMY8OzDi03SiXxk4vfyROrRzNiZAff/8VSlt87jmefHJV1aJnRUHjXvKB5JEQ7PPIpMeYYGP4OeMclwdP/rKxDrKu8PbVs5KTxDuCLEfFuYBZwrqTpDbxfXUybsY11a4fywjPD6GhvYvEt+zD7xM1Zh5Urr7QN44nVowF4fdsQnnlqFK0TdmQcVbYkaB5ZeR0dlUOCEQfD8IMyDa0xGrv6RWoNq5ElKzx2rvK4VdJqYBLwcKPuWQ/77t/OhnVD3zhvW9/CIUdtyzCifNvvgNd5xyFbeeTBMVmHkrkowepPih3PwviPwajDso6oQSJ/Ty37ZRkfSQcBM4D7++N+faFuWgA56w7IjeEjOrjg0pXM+85UXn/N3a1qhum/CA67LXhtJbz+eNYRNVD9Nh+pi4YnMkl7ATcBX4iILd28f5akpZKWtpN986RtfQvjD9j5xnnrxHY2vtCSYUT51DykzAWXrmTxbybwn4vGZx1OrgwZDaNnBpv/M+tIGkcRNR39paGJTFILlSR2bUTc3N1nImJeRMyMiJktDGtkODVZs2Ikk6bsZMLkHQxpKTPnlE3cd/veWYeVM8EXvvkIzz45igU/OzDrYHKh/WXo2Fp5Xd4OW+/XwOwb6zRY+sgkCbgKWB0R323UfeqtXBJXXDCJi657kqZmuP2GcTz9qJ9YdjV9xmY+8Dcv8tSjo7jsxiUAXPP9g1l6974ZR5ad9jZY+zVBGaIMY08I9jkWXrkDnv2O6HgFHv+cGDkNpv6g4H0VQWUcQo40smPjGOB04CFJK5JrX4mI3zbwnnWx5I4xLLnDndd78vCf9+Hkw47LOoxcGfkumH7D7glq7PEw9viCJ65diP5tNtaikU8t76EyLcvMBppyvqpkftRkZunUsWkpaS2wFSgBHb3dA9OJzMxSq3PT8riIaOtLAU5kZpZezvrIvK+lmaWUaoPe1s5xoslx1u6FcbukZd28VzPXyMwsnXS7KLVV6fc6JiLWSdoPWCjpkYi4K21IrpGZWWr1GtkfEeuSP18CFgBH9yYeJzIzS68OI/sljZI0uvM18CFgZW/CcdPSzNIJoFyXzv4JwILKJCCGANdFxO97U5ATmZmlVJ95lBHxJHBE3+NxIjOz3sjZ8AsnMjNLJ4CSpyiZWaFFZYmPHHEiM7P03LQ0s0Kr31PLunEiM7P0XCMzs8JzIjOzQouAUinrKN7CiczM0nONzMwKz4nMzIot/NTSzAouIDwg1swKz1OUzKzQIrwdnJkNAO7sN7OiC9fIzKzY6rOwYj05kZlZOp40bmZFF0DkbIqSd1Eys3QiWVixlqMKSXMlrZH0uKTzexuSa2RmllrUoWkpqRm4AjgBeA5YIunWiHg4bVmukZlZevWpkR0NPB4RT0bETuAG4JTehKPI0dMHSRuAp7OOo4tWoC3rIHLMv091efuN3h4R4/tSgKTfU/nnqsVwYHuX83kRMS8p5yPA3Ij4dHJ+OvDeiDgvbUy5alr29QeuN0lLI2Jm1nHklX+f6gbibxQRc+tUlLorvjcFuWlpZll5Dpjc5fxtwLreFOREZmZZWQJMlTRF0lDg48CtvSkoV03LHJqXdQA559+nOv9GexARHZLOA24DmoGrI2JVb8rKVWe/mVlvuGlpZoXnRGZmhedE1o16TZsYqCRdLeklSSuzjiWPJE2WdKek1ZJWSfp81jENdO4j20UybeJRukybAD7Rm2kTA5WkY4FXgZ9FxKFZx5M3kiYCEyNiuaTRwDLgw/471Diuke2ubtMmBqqIuAt4Oes48ioi1kfE8uT1VmA1MCnbqAY2J7LdTQKe7XL+HP5LaL0k6SBgBnB/xqEMaE5ku6vbtAkb3CTtBdwEfCEitmQdz0DmRLa7uk2bsMFLUguVJHZtRNycdTwDnRPZ7uo2bcIGJ0kCrgJWR8R3s45nMHAi20VEdACd0yZWA/N7O21ioJJ0PXAvME3Sc5LOzDqmnDkGOB04XtKK5Dg566AGMg+/MLPCc43MzArPiczMCs+JzMwKz4nMzArPiczMCs+JrEAklZJH+Ssl3ShpZB/K+mmyiw2SfiJpeg+fnSPpfb24x1pJu+22s6fru3zm1ZT3+oakL6WN0QYGJ7JieT0ijkxWnNgJnN31zWTljtQi4tNVVmaYA6ROZGb9xYmsuO4G3pnUlu6UdB3wkKRmSf9X0hJJD0r6LFRGm0u6XNLDkn4D7NdZkKTFkmYmr+dKWi7pAUmLkknPZwP/kNQG/1LSeEk3JfdYIumY5Lv7Srpd0p8l/Yju562+haT/J2lZsm7XWbu8d0kSyyJJ45Nr75D0++Q7d0s6pC6/phVbRPgoyAG8mvw5BLgFOIdKbek1YEry3lnAV5PXw4ClwBTgb4GFVDZ5OADYBHwk+dxiYCYwnsrKH51ljUv+/AbwpS5xXAe8P3l9IJWpOADfB76WvP4rKpPtW7v551jbeb3LPUYAK4F9k/MATktefw24PHm9CJiavH4vcEd3MfoYXId3USqWEZJWJK/vpjKf733AnyLiqeT6h4DDO/u/gL2BqcCxwPURUQLWSbqjm/JnAXd1lhURe1pz7IPA9MqUQgDGJAsIHkslYRIRv5H0Sg3/TJ+TdGryenIS60agDPwiuf7vwM3JahLvA27scu9hNdzDBjgnsmJ5PSKO7Hoh+Q/6ta6XgL+PiNt2+dzJVF+OSDV8BipdErMj4vVuYql5zpukOVSS4uyI2CZpMTB8Dx+P5L6bdv0NzNxHNvDcBpyTLCODpHdJGgXcBXw86UObCBzXzXfvBf6LpCnJd8cl17cCo7t87nYqE+tJPndk8vIu4LTk2knA2Cqx7g28kiSxQ6jUCDs1AZ21yk8C90RlTa+nJH00uYckHVHlHjYIOJENPD8BHgaWJ5uD/IhKzXsB8BjwEHAl8B+7fjEiNlDpY7tZ0gO82bT7FXBqZ2c/8DlgZvIw4WHefHr6TeBYScupNHGfqRLr74Ehkh4ELgTu6/Lea8B7JC0Djge+lVw/DTgziW8VXobc8OoXZjYAuEZmZoXnRGZmhedEZmaF50RmZoXnRGZmhedEZmaF50RmZoX3/wHghFewCNteuQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can also plot the confusion matrix\n",
    "# Note: more recent sklearn provides a function for this \n",
    "metrics.plot_confusion_matrix(classifier, X_train, y_train)\n",
    "\n",
    "# In case your version does not have it\n",
    "#sn.set(font_scale=1.4) # for label size\n",
    "#sn.heatmap(metrics.confusion_matrix(y_test, y_test_pred), \n",
    "#           annot=True, annot_kws={\"size\": 16}) # font size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have just used a logistic regression model as a classifier. Other popular classifiers are \n",
    "- decision trees  ([detail](https://en.wikipedia.org/wiki/Decision_tree_learning))\n",
    "- nearest neighbor methods ([detail](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) and [overview](https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn))\n",
    "- support-vector machine (SVM) ([detail](https://en.wikipedia.org/wiki/Support_vector_machine ))\n",
    "- Random Forest method ([detail](https://en.wikipedia.org/wiki/Random_forest) )\n",
    "\n",
    "See a [flowchart](https://scikit-learn.org/stable/tutorial/machine_learning_map/) of a rough guide on how to choose an estimator and detailed [comparison](https://www.dataschool.io/comparing-supervised-learning-algorithms/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18,  0,  0],\n",
       "       [ 0, 10,  0],\n",
       "       [ 0,  3, 14]], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a decision tree as classifier\n",
    "classifier = tree.DecisionTreeClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_test_pred = classifier.predict(X_test)\n",
    "metrics.confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the decision tree\n",
    "tree.plot_tree(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the nearest neighbor classifier\n",
    "classifier = neighbors.KNeighborsClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_test_pred = classifier.predict(X_test)\n",
    "metrics.confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SVM classifer --- next class is devoted to SVM\n",
    "classifier = svm.SVC(gamma=\"auto\")\n",
    "classifier.fit(X_train, y_train)\n",
    "y_test_pred = classifier.predict(X_test)\n",
    "metrics.confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Random Forest classifer\n",
    "classifier = ensemble.RandomForestClassifier(n_estimators=100)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_test_pred = classifier.predict(X_test)\n",
    "metrics.confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the feature importances --- we again see the petal width/height are most informative\n",
    "classifier.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will vary the training and testing sample sizes and compare four classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a numpy array with training size ratios, ranging from 10% to 90%\n",
    "train_size_vec = np.linspace(0.1, 0.9, 30)\n",
    "\n",
    "# create a list of classifiers\n",
    "classifiers = [tree.DecisionTreeClassifier,\n",
    "               neighbors.KNeighborsClassifier,\n",
    "               svm.SVC,\n",
    "               ensemble.RandomForestClassifier\n",
    "              ]\n",
    "\n",
    "# create an array that stores the diagonals of the confusion matrix as a function of training size ratio\n",
    "# and classifier\n",
    "cm_diags = np.zeros((3, len(train_size_vec), len(classifiers)), dtype=float)\n",
    "\n",
    "# loop over each training size ratio and classifier\n",
    "for n, train_size in enumerate(train_size_vec):\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        model_selection.train_test_split(iris.data, iris.target, train_size=train_size)\n",
    "\n",
    "    for m, Classifier in enumerate(classifiers): \n",
    "        classifier = Classifier()\n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_test_pred = classifier.predict(X_test)\n",
    "        cm_diags[:, n, m] = metrics.confusion_matrix(y_test, y_test_pred).diagonal()\n",
    "        cm_diags[:, n, m] /= np.bincount(y_test)\n",
    "        \n",
    "# plot accuracy as a function of training size ratio\n",
    "fig, axes = plt.subplots(1, len(classifiers), figsize=(12, 3))\n",
    "\n",
    "for m, Classifier in enumerate(classifiers): \n",
    "    axes[m].plot(train_size_vec, cm_diags[2, :, m], label=iris.target_names[2])\n",
    "    axes[m].plot(train_size_vec, cm_diags[1, :, m], label=iris.target_names[1])\n",
    "    axes[m].plot(train_size_vec, cm_diags[0, :, m], label=iris.target_names[0])\n",
    "    axes[m].set_title(type(Classifier()).__name__,fontsize=\"x-small\")\n",
    "    axes[m].set_ylim(0, 1.1)\n",
    "    axes[m].set_xlim(0.1, 0.9)\n",
    "    axes[m].set_ylabel(\"classification accuracy\",fontsize=\"x-small\")\n",
    "    axes[m].set_xlabel(\"training size ratio\",fontsize=\"x-small\")\n",
    "    axes[m].legend(loc=4,fontsize=\"x-small\")\n",
    "    axes[m].tick_params(axis=\"x\", labelsize=12)\n",
    "    axes[m].tick_params(axis=\"y\", labelsize=12)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that the error is different for each model.\n",
    "- Which classifier is best depends on the problem.\n",
    "- The good news is that it's easy to switch them in Scikit-learn.\n",
    "- Besides accuracy, computational performance can be important.  For large problems with many features, a decision tree method such as Random Forest is a good one to try first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "- Clustering can be considered as a classification problem where the classes are NOT known. For more details, see [Wikipedia](https://en.wikipedia.org/wiki/Cluster_analysis)).\n",
    "- It is an example of unsupervised learning (data is unlabeled). \n",
    "- The input of a clustering algorithm contains only the feature variables and the output of the algorithm is an array of integers that represent a cluster(or class) of each sample.\n",
    "- Popular clustering methods are:\n",
    "    - [*K-means algorithm*](https://en.wikipedia.org/wiki/K-means_clustering): groups the samples into clusters such that the within-group sum of square deviation is minimized.  ( `sklearn.cluster.Kmeans`)\n",
    "    - [*mean-shift algorithm*](https://en.wikipedia.org/wiki/Mean_shift) : clusters the samples by fitting the data to density functions (e.g. Gaussian functions)  ( `sklearn.cluster.MeanShift`)\n",
    "\n",
    " A full list of methods in Scikit-Learn [here](http://scikit-learn.org/stable/modules/clustering.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** Consider again the Iris dataset but this time we will not use the response variable. We will implement the K-means method. We need to specify the number of clusters (we will use `n_clusters = 3` since we know this in advance). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store feature data in X and response data in y\n",
    "X, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed so that we get the same random numbers\n",
    "np.random.seed(555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1: create an instance of KMeans class using number of clusters = 3\n",
    "clustering = cluster.KMeans(n_clusters = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2: call the fit() method\n",
    "clustering.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step3: use predict() method to make prediction\n",
    "y_pred = clustering.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the output is long, we'll look at every 8th element\n",
    "y_pred[::8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[::8]\n",
    "## NOTE: there is a good correlation btw the two, but the output has assigned different numbers to the groups\n",
    "##   than what was used in the target vector\n",
    "## - To be able to compare two arrays with metrics such as the confusion matrix, we need to rename the elements\n",
    "##      so that the same integers are used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the elements in y_pred so that the same integers are used as in y\n",
    "idx_0, idx_1, idx_2 = (np.where(y_pred == n) for n in range(3))\n",
    "y_pred[idx_0], y_pred[idx_1], y_pred[idx_2] = 0, 2, 1\n",
    "y_pred[::8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the confusion matrix\n",
    "metrics.confusion_matrix(y, y_pred)\n",
    "\n",
    "## NOTE(numbers might be different): the algorithm was able to correctly identify all samples in group 0 (first species) as a group of its own.\n",
    "#  2 elements from group 1 was assigned to group 2\n",
    "# 14 elements from group 2 was assinged to group 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make scatter plots for each pair of features\n",
    "\n",
    "N = X.shape[1]\n",
    "\n",
    "fig, axes = plt.subplots(N, N, figsize=(12, 12), sharex=True, sharey=True)\n",
    "\n",
    "colors = [\"coral\", \"blue\", \"green\"] # different color for each cluster\n",
    "markers = [\"^\", \"v\", \"o\"]           # different symbol for each cluster\n",
    "n_clusters = 3\n",
    "for m in range(N):\n",
    "    for n in range(N):\n",
    "        for p in range(n_clusters):\n",
    "            mask = y_pred == p\n",
    "            axes[m, n].scatter(X[:, m][mask], X[:, n][mask],\n",
    "                               marker=markers[p], s=30, \n",
    "                               color=colors[p], alpha=0.25) # alpha is transparency\n",
    "\n",
    "        for idx in np.where(y != y_pred):  # Put a red rectangle at bad predictions\n",
    "            axes[m, n].scatter(X[idx, m], X[idx, n],\n",
    "                               marker=\"s\", s=30, \n",
    "                               edgecolor=\"red\", \n",
    "                               facecolor=(1,1,1,0))\n",
    "        axes[m,n].set_xlim([0,8])\n",
    "        axes[m,n].set_ylim([0,8])\n",
    "        axes[m,n].set_xticks([0,2,4,6,8])\n",
    "        axes[m,n].set_yticks([0,2,4,6,8])\n",
    "\n",
    "    axes[N-1, m].set_xlabel(iris.feature_names[m], fontsize=16)\n",
    "    axes[m, 0].set_ylabel(iris.feature_names[m], fontsize=16)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"clustering_iris.pdf\")\n",
    "## NOTE: the clustering does a very good job at recognizing which sample belongs to distinct group,\n",
    "## but because of the overlap in the features we cannot expect any unsupervised clustering algorithm can\n",
    "## fully resolve the various groups in the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References: \n",
    "- *Numerical Python: A Practical Techniques Approach for Industry*  by Robert Johansson (Chapter 15)\n",
    "- *Python Data Science Handbook* by Jake VanderPlas\n",
    "- https://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
